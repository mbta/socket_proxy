# SocketProxy

A simple app that listens on a port and proxies (and possibly fans out) data to multiple given destinations.

## Usage

```
$ mix socket_proxy --listen-port 8001 12.12.12.12:8002 13.13.13.13:8003
```

## Testing

This app comes with a fake source which generates random strings and fake servers which can serve as the destinations. Here's how to set them up.

Open several terminal windows, all in the socket_proxy directory.

Start up a couple servers on different ports:

```
$ mix fake_server --port 8080  # in one window
$ mix fake_server --port 8081  # in another window
```

In another terminal window, start up the source data which will attempt to listen on a port and then start sending random bytes once the connection is made.

```
$ mix fake_source --port 8000
```

You can then run socket proxy to connect them all:

```
$ mix socket_proxy --listen-port 8000 127.0.0.1:8080 127.0.0.1:8081
```

Observe that data being generated by the fake source appears in all the fake servers. Try killing and restarting any combination of the windows and see that the data continues to flow.

## Architecture

The `socket_proxy` app is made up of three types of processes in a supervision tree: A `Listener` which listens on a port for the source data, multiple `Sender`s, one for each proxy destination, and one `Dispatcher` which connects the listener to the senders.

### Listener

The listener is a TCP socket server that listens on a provided port and waits for an incoming connection to accept.

A connection is accepted in `{:active, :once}` mode which means the next TCP packets will be sent to the process as a `{:tcp, _port, data}` message _once_, and then the socket will switch to `{:active, false}` (i.e. passive mode) where further data will buffer (somewhere?) and the process needs to either change the socket back to active mode or explicitly `recv/2,3` data. The `Listener` has a `handle_info({:tcp,..,..})` callback to receive the data, and once it does it uses `:inet.setopts(socket, active: :once)` to put it back in active mode for one more packet. This follows the example given in the [`gen_tcp` documentation](http://erlang.org/doc/man/gen_tcp.html#examples). For more information on the `:active` flag, see the [`inet` documentation](http://erlang.org/doc/man/inet.html#setopts-2).

The benefit of using `:active` mode is that receiving data as messages works more cleanly in a GenServer in a supervision tree. Otherwise, `:gen_tcp` wants to get in a `recv` loop where it receives from a socket, does something with the data, and then loops again and tries to receive again, which takes the process out of the GenServer loop. On the other hand, the documentation warns that leaving the socket as `{:active, true}` can overwhelm the process with messages - there's no back pressure. So setting it to `{:active, :once}`, receiving the message, and then setting it back to `{:active, :once}` seems to be a good compromise.

If the process receives a `{:tcp_closed, _}` message, then it will re-enter the accepting loop it started in.

In order to prevent duplicate messages, `Listener` only accepts a single connection on the socket at a time, although multiple connections to the socket server are supported by the TCP. But in our use case, there _shouldn't_ be multiple connections at once.

When the `Listener` receives a `{:tcp, _port, data}` message, it calls its `dispatcher_fn` with the data. The `dispatcher_fn` can be provided to the `Listener` at start-up time, and by default is given `&SocketProxy.Dispatcher.new_data/1`. It expects the function to run quickly! Since this is a blocking call in the process, we don't want it to hang up here and not receive TCP data on the port it's listening to. For example, we should not attempt to send this data to AWS synchronously from here.

### Dispatcher

The dispatcher is a simple GenServer that keeps a list of callback process IDs in its state. It has a `register/2` function that processes can use to add their ID to its list.

When `new_data/1,2` is invoked, the data is sent as a `{:new_data, data}` message to each of the pids in its callback list.

Recall that `new_data` needs to be fast from the `Listener` point of view. Sending a message is fast, and that's where the synchronous call from `Listener` ends. Also note that sending a message to a dead process is a no-op in BEAM, so if one of the callback processes dies, even though its pid remains in the dispatcher's list, it shouldn't be an issue to send to it. Although, at the moment, the supervision tree is set up as `one_for_all` for simplicity, it probably shouldn't the case that one of the callback_pids is dead.

### Sender

There is a sender process for each destination provided at app start up. The isolation is designed to keep the app sending to the other destinations even if one starts acting up.

The dispatcher sends a `{:new_data, data}` message to each sender whenever it receives data from the listener. So far the responses have been fast, in order to not block the listener from receiving messages. However, given that the internet can be unreliable and unpredictable, here at last is the back-pressure strategy. Each sender process individually keeps a buffer of messages, and the `new_data` callback simply preprends to it, and culls to 500. If sending data out is an issue, this process may drop messages once its buffer fills up, while other processes with a better connection should keep up.

There is a separate every-200-ms process that loops, takes the messages out of the buffer, and sends them along.
